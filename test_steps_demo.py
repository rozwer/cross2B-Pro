#!/usr/bin/env python3
"""Demo script to test each workflow step individually.

This script tests the LLM-based steps without requiring the full
Temporal/Tools infrastructure.
"""

import asyncio
import json
import os
import sys
from typing import Any

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))


def print_section(title: str, char: str = "=") -> None:
    """Print a section header."""
    print(f"\n{char * 60}")
    print(f" {title}")
    print(f"{char * 60}\n")


async def test_step2_csv_validation() -> dict[str, Any]:
    """Test Step 2: CSV Validation (no LLM required)."""
    print_section("Step 2: CSV Validation")

    from apps.api.validation.schemas import ValidationSeverity

    # Simulate step1 competitor data
    competitors = [
        {
            "url": "https://example.com/article1",
            "title": "SEOè¨˜äº‹ã®æ›¸ãæ–¹å®Œå…¨ã‚¬ã‚¤ãƒ‰",
            "content": "SEOè¨˜äº‹ã‚’æ›¸ãéš›ã«ã¯ã€ã¾ãšã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é¸å®šãŒé‡è¦ã§ã™ã€‚" * 10,
        },
        {
            "url": "https://example.com/article2",
            "title": "2024å¹´ã®SEOãƒˆãƒ¬ãƒ³ãƒ‰",
            "content": "æœ€æ–°ã®SEOãƒˆãƒ¬ãƒ³ãƒ‰ã«ã¤ã„ã¦è§£èª¬ã—ã¾ã™ã€‚" * 15,
        },
        {
            "url": "https://example.com/article3",
            "title": "",  # Empty title - should generate warning
            "content": "çŸ­ã„",  # Too short - should generate warning
        },
    ]

    # Validation logic (inline from step2)
    validated_records = []
    validation_issues = []
    required_fields = ["url", "title", "content"]

    for idx, competitor in enumerate(competitors):
        record_issues = []

        for field in required_fields:
            if field not in competitor or not competitor[field]:
                record_issues.append({
                    "field": field,
                    "issue": "missing_or_empty",
                    "severity": ValidationSeverity.ERROR.value,
                })

        content = competitor.get("content", "")
        if len(content) < 100:
            record_issues.append({
                "field": "content",
                "issue": "content_too_short",
                "severity": ValidationSeverity.WARNING.value,
                "value": len(content),
            })

        if record_issues:
            validation_issues.append({
                "index": idx,
                "url": competitor.get("url", "unknown"),
                "issues": record_issues,
            })
        else:
            validated_records.append(competitor)

    result = {
        "step": "step2",
        "is_valid": len(validated_records) > 0,
        "total_records": len(competitors),
        "valid_records": len(validated_records),
        "validation_issues": validation_issues,
    }

    print(f"âœ… Total records: {result['total_records']}")
    print(f"âœ… Valid records: {result['valid_records']}")
    print(f"âš ï¸  Issues found: {len(validation_issues)}")
    for issue in validation_issues:
        print(f"   - Record {issue['index']} ({issue['url']}): {len(issue['issues'])} issues")

    return result


async def test_step3a_query_analysis() -> dict[str, Any]:
    """Test Step 3A: Query Analysis using Gemini."""
    print_section("Step 3A: Query Analysis (Gemini)")

    from apps.api.llm import GeminiClient
    from apps.api.llm.schemas import LLMRequestConfig

    client = GeminiClient()
    keyword = "AIæ´»ç”¨SEOè¨˜äº‹ä½œæˆ"

    prompt = f"""ä»¥ä¸‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«é–¢ã™ã‚‹æ¤œç´¢ã‚¯ã‚¨ãƒªåˆ†æã‚’è¡Œã£ã¦ãã ã•ã„ã€‚

ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keyword}

ä»¥ä¸‹ã®é …ç›®ã«ã¤ã„ã¦åˆ†æã—ã¦ãã ã•ã„ï¼š
1. æ¤œç´¢æ„å›³ï¼ˆæƒ…å ±åé›†ã€æ¯”è¼ƒæ¤œè¨ã€è³¼å…¥æ„å›³ãªã©ï¼‰
2. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒšãƒ«ã‚½ãƒŠï¼ˆå¹´é½¢å±¤ã€è·æ¥­ã€èª²é¡Œï¼‰
3. é–¢é€£ã™ã‚‹ã‚µãƒ–ã‚¯ã‚¨ãƒªï¼ˆ5ã¤ï¼‰
4. ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã§è§£æ±ºã™ã¹ãèª²é¡Œ

JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"""

    config = LLMRequestConfig(max_tokens=2000, temperature=0.7)
    response = await client.generate(
        messages=[{"role": "user", "content": prompt}],
        system_prompt="You are a search query analysis expert.",
        config=config,
    )

    result = {
        "step": "step3a",
        "keyword": keyword,
        "query_analysis": response.content[:500] + "..." if len(response.content) > 500 else response.content,
        "model": response.model,
        "tokens": {
            "input": response.token_usage.input,
            "output": response.token_usage.output,
        },
    }

    print(f"âœ… Model: {result['model']}")
    print(f"âœ… Tokens: input={result['tokens']['input']}, output={result['tokens']['output']}")
    print(f"\nğŸ“ Analysis preview:\n{result['query_analysis'][:400]}...")

    return result


async def test_step3b_cooccurrence() -> dict[str, Any]:
    """Test Step 3B: Co-occurrence Extraction using Gemini."""
    print_section("Step 3B: Co-occurrence Extraction (Gemini)")

    from apps.api.llm import GeminiClient
    from apps.api.llm.schemas import LLMRequestConfig

    client = GeminiClient()
    keyword = "AIæ´»ç”¨SEOè¨˜äº‹ä½œæˆ"

    # Simulated competitor content
    competitors = [
        {"title": "AIã§SEOè¨˜äº‹ã‚’è‡ªå‹•ç”Ÿæˆã™ã‚‹æ–¹æ³•", "content_preview": "ChatGPTã‚„Geminiã‚’æ´»ç”¨ã—ã¦ã€SEOæœ€é©åŒ–ã•ã‚ŒãŸè¨˜äº‹ã‚’åŠ¹ç‡çš„ã«ä½œæˆã™ã‚‹æ‰‹æ³•ã«ã¤ã„ã¦è§£èª¬ã—ã¾ã™ã€‚"},
        {"title": "2024å¹´SEOè¨˜äº‹ã®æ›¸ãæ–¹ã‚¬ã‚¤ãƒ‰", "content_preview": "æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³æœ€é©åŒ–ã®æœ€æ–°ãƒˆãƒ¬ãƒ³ãƒ‰ã¨ã€AIãƒ„ãƒ¼ãƒ«ã‚’æ´»ç”¨ã—ãŸåŠ¹æœçš„ãªè¨˜äº‹ä½œæˆã®ãƒã‚¤ãƒ³ãƒˆã€‚"},
    ]

    prompt = f"""ä»¥ä¸‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ç«¶åˆè¨˜äº‹ã‹ã‚‰å…±èµ·èªãƒ»é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keyword}

ç«¶åˆè¨˜äº‹:
{json.dumps(competitors, ensure_ascii=False, indent=2)}

ä»¥ä¸‹ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ï¼š
1. ä¸»è¦å…±èµ·èªï¼ˆ5-10å€‹ï¼‰
2. LSIã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆæ½œåœ¨æ„å‘³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰
3. é–¢é€£ã™ã‚‹å°‚é–€ç”¨èª
4. ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒä¸€ç·’ã«æ¤œç´¢ã—ãã†ãªèªå¥

JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"""

    config = LLMRequestConfig(max_tokens=2000, temperature=0.5)
    response = await client.generate(
        messages=[{"role": "user", "content": prompt}],
        system_prompt="You are a co-occurrence keyword analysis expert.",
        config=config,
    )

    result = {
        "step": "step3b",
        "keyword": keyword,
        "cooccurrence_analysis": response.content[:500] + "..." if len(response.content) > 500 else response.content,
        "model": response.model,
        "tokens": {
            "input": response.token_usage.input,
            "output": response.token_usage.output,
        },
    }

    print(f"âœ… Model: {result['model']}")
    print(f"âœ… Tokens: input={result['tokens']['input']}, output={result['tokens']['output']}")
    print(f"\nğŸ“ Co-occurrence preview:\n{result['cooccurrence_analysis'][:400]}...")

    return result


async def test_step3c_competitor_analysis() -> dict[str, Any]:
    """Test Step 3C: Competitor Analysis using Gemini."""
    print_section("Step 3C: Competitor Analysis (Gemini)")

    from apps.api.llm import GeminiClient
    from apps.api.llm.schemas import LLMRequestConfig

    client = GeminiClient()
    keyword = "AIæ´»ç”¨SEOè¨˜äº‹ä½œæˆ"

    competitors = [
        {"url": "https://example.com/ai-seo-1", "title": "AIã§SEOè¨˜äº‹ã‚’ä½œã‚‹", "content_length": 3500},
        {"url": "https://example.com/ai-seo-2", "title": "ChatGPTè¨˜äº‹ä½œæˆè¡“", "content_length": 4200},
        {"url": "https://example.com/ai-seo-3", "title": "SEOè¨˜äº‹è‡ªå‹•åŒ–ã‚¬ã‚¤ãƒ‰", "content_length": 2800},
    ]

    prompt = f"""ä»¥ä¸‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ç«¶åˆè¨˜äº‹ã‚’åˆ†æã—ã€å·®åˆ¥åŒ–æˆ¦ç•¥ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚

ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keyword}

ç«¶åˆè¨˜äº‹:
{json.dumps(competitors, ensure_ascii=False, indent=2)}

ä»¥ä¸‹ã‚’åˆ†æã—ã¦ãã ã•ã„ï¼š
1. ç«¶åˆã®å¼·ã¿ãƒ»å¼±ã¿
2. å·®åˆ¥åŒ–ãƒã‚¤ãƒ³ãƒˆï¼ˆ3-5ã¤ï¼‰
3. ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚®ãƒ£ãƒƒãƒ—ï¼ˆç«¶åˆãŒæ‰±ã£ã¦ã„ãªã„ãƒˆãƒ”ãƒƒã‚¯ï¼‰
4. æ¨å¥¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æˆ¦ç•¥

JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"""

    config = LLMRequestConfig(max_tokens=2000, temperature=0.7)
    response = await client.generate(
        messages=[{"role": "user", "content": prompt}],
        system_prompt="You are a competitor analysis expert.",
        config=config,
    )

    result = {
        "step": "step3c",
        "keyword": keyword,
        "competitor_analysis": response.content[:500] + "..." if len(response.content) > 500 else response.content,
        "model": response.model,
        "tokens": {
            "input": response.token_usage.input,
            "output": response.token_usage.output,
        },
    }

    print(f"âœ… Model: {result['model']}")
    print(f"âœ… Tokens: input={result['tokens']['input']}, output={result['tokens']['output']}")
    print(f"\nğŸ“ Competitor analysis preview:\n{result['competitor_analysis'][:400]}...")

    return result


async def test_step5_primary_sources() -> dict[str, Any]:
    """Test Step 5: Primary Source Collection (simulated)."""
    print_section("Step 5: Primary Source Collection (Simulated)")

    # Since we don't have actual SERP tools, simulate the output
    sources = [
        {
            "url": "https://research.google/blog/ai-content-quality",
            "title": "Google Research on AI Content Quality",
            "excerpt": "Latest research on how AI-generated content is evaluated...",
            "verified": True,
        },
        {
            "url": "https://searchengineland.com/seo-best-practices-2024",
            "title": "SEO Best Practices 2024",
            "excerpt": "Comprehensive guide to SEO optimization techniques...",
            "verified": True,
        },
    ]

    result = {
        "step": "step5",
        "keyword": "AIæ´»ç”¨SEOè¨˜äº‹ä½œæˆ",
        "sources": sources,
        "search_queries": ["AI SEO research statistics", "SEO content quality studies"],
        "total_collected": len(sources),
        "total_verified": len([s for s in sources if s.get("verified")]),
    }

    print(f"âœ… Sources collected: {result['total_collected']}")
    print(f"âœ… Sources verified: {result['total_verified']}")
    for s in sources:
        print(f"   - {s['title']}")

    return result


async def test_step6_enhanced_outline() -> dict[str, Any]:
    """Test Step 6: Enhanced Outline using Anthropic."""
    print_section("Step 6: Enhanced Outline (Anthropic)")

    from apps.api.llm import AnthropicClient
    from apps.api.llm.schemas import LLMRequestConfig

    client = AnthropicClient()
    keyword = "AIæ´»ç”¨SEOè¨˜äº‹ä½œæˆ"

    # Simulated step4 outline
    basic_outline = """
1. AIÃ—SEOè¨˜äº‹ä½œæˆã®æ¦‚è¦
2. AIãƒ„ãƒ¼ãƒ«ã®é¸ã³æ–¹
3. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆã®ã‚³ãƒ„
4. SEOæœ€é©åŒ–ã®ãƒã‚¤ãƒ³ãƒˆ
5. å“è³ªç®¡ç†ã®æ–¹æ³•
6. äº‹ä¾‹ç´¹ä»‹
7. ã¾ã¨ã‚
"""

    sources = [
        {"title": "Google Research on AI", "url": "https://research.google"},
        {"title": "SEO Best Practices", "url": "https://searchengineland.com"},
    ]

    prompt = f"""ä»¥ä¸‹ã®åŸºæœ¬ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ãƒ³ã‚’ã€ä¸€æ¬¡æƒ…å ±ã‚’çµ„ã¿è¾¼ã‚“ã§å¼·åŒ–ã—ã¦ãã ã•ã„ã€‚

ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keyword}

åŸºæœ¬ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ãƒ³:
{basic_outline}

å‚ç…§å¯èƒ½ãªä¸€æ¬¡æƒ…å ±:
{json.dumps(sources, ensure_ascii=False, indent=2)}

ä»¥ä¸‹ã®ç‚¹ã‚’å¼·åŒ–ã—ã¦ãã ã•ã„ï¼š
1. å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«å…·ä½“çš„ãªã‚µãƒ–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
2. ãƒ‡ãƒ¼ã‚¿ã‚„çµ±è¨ˆã‚’å¼•ç”¨ã™ã‚‹ãƒã‚¤ãƒ³ãƒˆã‚’æ˜ç¤º
3. èª­è€…ã®ç–‘å•ã«ç­”ãˆã‚‹æ§‹æˆ
4. E-E-A-Tè¦ç´ ã®çµ„ã¿è¾¼ã¿

å¼·åŒ–ã•ã‚ŒãŸã‚¢ã‚¦ãƒˆãƒ©ã‚¤ãƒ³ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚"""

    config = LLMRequestConfig(max_tokens=3000, temperature=0.6)
    response = await client.generate(
        messages=[{"role": "user", "content": prompt}],
        system_prompt="You are an SEO content outline specialist.",
        config=config,
    )

    result = {
        "step": "step6",
        "keyword": keyword,
        "enhanced_outline": response.content[:800] + "..." if len(response.content) > 800 else response.content,
        "model": response.model,
        "tokens": {
            "input": response.token_usage.input,
            "output": response.token_usage.output,
        },
    }

    print(f"âœ… Model: {result['model']}")
    print(f"âœ… Tokens: input={result['tokens']['input']}, output={result['tokens']['output']}")
    print(f"\nğŸ“ Enhanced outline preview:\n{result['enhanced_outline'][:600]}...")

    return result


async def test_step6_5_integration() -> dict[str, Any]:
    """Test Step 6.5: Integration Package using Anthropic."""
    print_section("Step 6.5: Integration Package (Anthropic)")

    from apps.api.llm import AnthropicClient
    from apps.api.llm.schemas import LLMRequestConfig

    client = AnthropicClient()
    keyword = "AIæ´»ç”¨SEOè¨˜äº‹ä½œæˆ"

    prompt = f"""ä»¥ä¸‹ã®åˆ†æçµæœã‚’çµ±åˆã—ã€è¨˜äº‹åŸ·ç­†ç”¨ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keyword}

ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ: SEOè¨˜äº‹ä½œæˆã«AIã‚’æ´»ç”¨ã™ã‚‹æ–¹æ³•ã«ã¤ã„ã¦ã®æƒ…å ±åé›†ç›®çš„
ã‚¯ã‚¨ãƒªåˆ†æ: ãƒãƒ¼ã‚±ã‚¿ãƒ¼ã€ãƒ–ãƒ­ã‚¬ãƒ¼ã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ¶ä½œè€…å‘ã‘
å…±èµ·ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: ChatGPT, Gemini, ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ, è‡ªå‹•ç”Ÿæˆ, å“è³ªç®¡ç†
ç«¶åˆå·®åˆ¥åŒ–: å®Ÿè·µçš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹ã®æä¾›

ä»¥ä¸‹ã®å½¢å¼ã§JSONå‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
{{
    "integration_package": "çµ±åˆã•ã‚ŒãŸè¨˜äº‹ä½œæˆã‚¬ã‚¤ãƒ‰",
    "outline_summary": "ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ãƒ³ã®è¦ç´„",
    "section_count": ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ•°,
    "total_sources": å‚ç…§ã‚½ãƒ¼ã‚¹æ•°
}}"""

    config = LLMRequestConfig(max_tokens=2000, temperature=0.5)
    response = await client.generate(
        messages=[{"role": "user", "content": prompt}],
        system_prompt="You are an SEO content integration specialist.",
        config=config,
    )

    # Try to parse JSON
    try:
        parsed = json.loads(response.content)
        integration_package = parsed.get("integration_package", "")
        section_count = parsed.get("section_count", 0)
    except json.JSONDecodeError:
        integration_package = response.content[:500]
        section_count = 7

    result = {
        "step": "step6_5",
        "keyword": keyword,
        "integration_package": integration_package[:300] + "..." if len(str(integration_package)) > 300 else integration_package,
        "section_count": section_count,
        "model": response.model,
        "tokens": {
            "input": response.token_usage.input,
            "output": response.token_usage.output,
        },
    }

    print(f"âœ… Model: {result['model']}")
    print(f"âœ… Sections: {result['section_count']}")
    print(f"âœ… Tokens: input={result['tokens']['input']}, output={result['tokens']['output']}")

    return result


async def test_step7b_brush_up() -> dict[str, Any]:
    """Test Step 7B: Brush Up using Gemini."""
    print_section("Step 7B: Brush Up/Polish (Gemini)")

    from apps.api.llm import GeminiClient
    from apps.api.llm.schemas import LLMRequestConfig

    client = GeminiClient()
    keyword = "AIæ´»ç”¨SEOè¨˜äº‹ä½œæˆ"

    # Simulated draft from step7a
    draft = """
# AIæ´»ç”¨SEOè¨˜äº‹ä½œæˆã‚¬ã‚¤ãƒ‰

## ã¯ã˜ã‚ã«
AIã‚’ä½¿ã£ã¦SEOè¨˜äº‹ã‚’ä½œæˆã™ã‚‹æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚æœ€è¿‘ã®AIæŠ€è¡“ã®é€²æ­©ã«ã‚ˆã‚Šã€åŠ¹ç‡çš„ã«è¨˜äº‹ã‚’ä½œæˆã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚

## AIãƒ„ãƒ¼ãƒ«ã®é¸ã³æ–¹
ChatGPTã‚„Geminiãªã©ã€æ§˜ã€…ãªAIãƒ„ãƒ¼ãƒ«ãŒã‚ã‚Šã¾ã™ã€‚ç›®çš„ã«å¿œã˜ã¦é¸æŠã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚

## ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ›¸ãæ–¹
è‰¯ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ›¸ãã“ã¨ã§ã€ã‚ˆã‚Šè³ªã®é«˜ã„è¨˜äº‹ãŒç”Ÿæˆã§ãã¾ã™ã€‚å…·ä½“çš„ãªæŒ‡ç¤ºã‚’å‡ºã™ã“ã¨ãŒãƒã‚¤ãƒ³ãƒˆã§ã™ã€‚
"""

    prompt = f"""ä»¥ä¸‹ã®è¨˜äº‹ãƒ‰ãƒ©ãƒ•ãƒˆã‚’ç£¨ãä¸Šã’ã¦ãã ã•ã„ã€‚

ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keyword}

ãƒ‰ãƒ©ãƒ•ãƒˆ:
{draft}

ä»¥ä¸‹ã®ç‚¹ã‚’æ”¹å–„ã—ã¦ãã ã•ã„ï¼š
1. è‡ªç„¶ã§èª­ã¿ã‚„ã™ã„æ–‡ç« ã«
2. å°‚é–€æ€§ã‚’ä¿ã¡ã¤ã¤åˆ†ã‹ã‚Šã‚„ã™ã
3. å°å…¥éƒ¨åˆ†ã‚’é­…åŠ›çš„ã«
4. é©åˆ‡ãªæ¥ç¶šè©ã®ä½¿ç”¨

JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š
{{
    "polished": "ç£¨ãä¸Šã’ã‚‰ã‚ŒãŸè¨˜äº‹",
    "word_count": æ–‡å­—æ•°,
    "changes_made": ["å¤‰æ›´ç‚¹1", "å¤‰æ›´ç‚¹2"]
}}"""

    config = LLMRequestConfig(max_tokens=4000, temperature=0.8)
    response = await client.generate(
        messages=[{"role": "user", "content": prompt}],
        system_prompt="You are a content polishing expert.",
        config=config,
    )

    # Try to parse JSON
    try:
        parsed = json.loads(response.content)
        polished = parsed.get("polished", "")
        changes = parsed.get("changes_made", [])
    except json.JSONDecodeError:
        polished = response.content[:500]
        changes = ["Natural language improvements", "Flow enhancement"]

    result = {
        "step": "step7b",
        "keyword": keyword,
        "polished_preview": polished[:400] + "..." if len(str(polished)) > 400 else polished,
        "changes_made": changes[:3] if isinstance(changes, list) else ["Improvements made"],
        "model": response.model,
        "tokens": {
            "input": response.token_usage.input,
            "output": response.token_usage.output,
        },
    }

    print(f"âœ… Model: {result['model']}")
    print(f"âœ… Changes made: {len(result['changes_made'])}")
    print(f"âœ… Tokens: input={result['tokens']['input']}, output={result['tokens']['output']}")
    print(f"\nğŸ“ Polished preview:\n{result['polished_preview'][:300]}...")

    return result


async def test_step8_fact_check() -> dict[str, Any]:
    """Test Step 8: Fact Check using Gemini."""
    print_section("Step 8: Fact Check (Gemini)")

    from apps.api.llm import GeminiClient
    from apps.api.llm.schemas import LLMRequestConfig

    client = GeminiClient()
    keyword = "AIæ´»ç”¨SEOè¨˜äº‹ä½œæˆ"

    content = """
AIã‚’æ´»ç”¨ã—ãŸSEOè¨˜äº‹ä½œæˆã¯ã€2023å¹´ä»¥é™æ€¥é€Ÿã«æ™®åŠã—ã¦ã„ã¾ã™ã€‚
ChatGPTã‚„Geminiãªã©ã®LLMã¯ã€æœˆé–“æ•°å„„äººã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«åˆ©ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚
Googleæ¤œç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€AIã§ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è‡ªå‹•çš„ã«ãƒšãƒŠãƒ«ãƒ†ã‚£ã¨ã—ã¦æ‰±ã†ã‚ã‘ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
"""

    prompt = f"""ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®äº‹å®Ÿç¢ºèªã‚’è¡Œã„ã€FAQã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keyword}

ã‚³ãƒ³ãƒ†ãƒ³ãƒ„:
{content}

ä»¥ä¸‹ã‚’å®Ÿæ–½ã—ã¦ãã ã•ã„ï¼š
1. å«ã¾ã‚Œã‚‹ä¸»å¼µã‚’æŠ½å‡º
2. å„ä¸»å¼µã®æ¤œè¨¼
3. é–¢é€£ã™ã‚‹FAQã‚’3ã¤ç”Ÿæˆ

å›ç­”å½¢å¼:
CLAIMS: [ä¸»å¼µãƒªã‚¹ãƒˆ]
VERIFICATION: [æ¤œè¨¼çµæœ]
FAQ:
Q1: ...
A1: ...
"""

    config = LLMRequestConfig(max_tokens=2500, temperature=0.3)
    response = await client.generate(
        messages=[{"role": "user", "content": prompt}],
        system_prompt="You are a fact-checking specialist.",
        config=config,
    )

    has_contradictions = "contradiction" in response.content.lower() or "incorrect" in response.content.lower()

    result = {
        "step": "step8",
        "keyword": keyword,
        "verification_preview": response.content[:400] + "..." if len(response.content) > 400 else response.content,
        "has_contradictions": has_contradictions,
        "model": response.model,
        "tokens": {
            "input": response.token_usage.input,
            "output": response.token_usage.output,
        },
    }

    print(f"âœ… Model: {result['model']}")
    print(f"âœ… Contradictions found: {result['has_contradictions']}")
    print(f"âœ… Tokens: input={result['tokens']['input']}, output={result['tokens']['output']}")
    print(f"\nğŸ“ Verification preview:\n{result['verification_preview'][:350]}...")

    return result


async def test_step9_final_rewrite() -> dict[str, Any]:
    """Test Step 9: Final Rewrite using Anthropic."""
    print_section("Step 9: Final Rewrite (Anthropic)")

    from apps.api.llm import AnthropicClient
    from apps.api.llm.schemas import LLMRequestConfig

    client = AnthropicClient()
    keyword = "AIæ´»ç”¨SEOè¨˜äº‹ä½œæˆ"

    polished_content = """
# AIæ´»ç”¨SEOè¨˜äº‹ä½œæˆã®å®Œå…¨ã‚¬ã‚¤ãƒ‰

AIã‚’æ´»ç”¨ã—ãŸSEOè¨˜äº‹ä½œæˆã¯ã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ã«é©å‘½ã‚’ã‚‚ãŸã‚‰ã—ã¦ã„ã¾ã™ã€‚
æœ¬è¨˜äº‹ã§ã¯ã€å®Ÿè·µçš„ãªãƒã‚¦ãƒã‚¦ã‚’ãŠä¼ãˆã—ã¾ã™ã€‚
"""

    faq = """
Q: AIã§ä½œæˆã—ãŸè¨˜äº‹ã¯Googleã«ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’å—ã‘ã¾ã™ã‹ï¼Ÿ
A: ã„ã„ãˆã€é«˜å“è³ªãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã§ã‚ã‚Œã°å•é¡Œã‚ã‚Šã¾ã›ã‚“ã€‚
"""

    prompt = f"""ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æœ€çµ‚ãƒªãƒ©ã‚¤ãƒˆã—ã¦ãã ã•ã„ã€‚

ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keyword}

æœ¬æ–‡:
{polished_content}

FAQ:
{faq}

JSONå½¢å¼ã§ä»¥ä¸‹ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
{{
    "final_content": "æœ€çµ‚ç‰ˆè¨˜äº‹",
    "word_count": æ–‡å­—æ•°,
    "meta_description": "120æ–‡å­—ä»¥å†…ã®ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³",
    "internal_link_suggestions": ["é–¢é€£è¨˜äº‹ãƒªãƒ³ã‚¯å€™è£œ"]
}}"""

    config = LLMRequestConfig(max_tokens=4000, temperature=0.6)
    response = await client.generate(
        messages=[{"role": "user", "content": prompt}],
        system_prompt="Perform the final rewrite of the article.",
        config=config,
    )

    # Try to parse JSON
    try:
        parsed = json.loads(response.content)
        final_content = parsed.get("final_content", "")
        meta_desc = parsed.get("meta_description", "")
    except json.JSONDecodeError:
        final_content = response.content[:500]
        meta_desc = "AIæ´»ç”¨SEOè¨˜äº‹ä½œæˆã®å®Ÿè·µã‚¬ã‚¤ãƒ‰"

    result = {
        "step": "step9",
        "keyword": keyword,
        "final_content_preview": final_content[:400] + "..." if len(str(final_content)) > 400 else final_content,
        "meta_description": meta_desc,
        "model": response.model,
        "tokens": {
            "input": response.token_usage.input,
            "output": response.token_usage.output,
        },
    }

    print(f"âœ… Model: {result['model']}")
    print(f"âœ… Meta description: {result['meta_description'][:80]}...")
    print(f"âœ… Tokens: input={result['tokens']['input']}, output={result['tokens']['output']}")
    print(f"\nğŸ“ Final content preview:\n{result['final_content_preview'][:300]}...")

    return result


async def main() -> None:
    """Run all step tests."""
    print_section("SEO Article Generation Workflow - Step Tests", "=")

    results = {}

    # Step 2: CSV Validation (no LLM)
    try:
        results["step2"] = await test_step2_csv_validation()
    except Exception as e:
        print(f"âŒ Step 2 failed: {e}")

    # Step 3A: Query Analysis (Gemini)
    try:
        results["step3a"] = await test_step3a_query_analysis()
    except Exception as e:
        print(f"âŒ Step 3A failed: {e}")

    # Step 3B: Co-occurrence (Gemini)
    try:
        results["step3b"] = await test_step3b_cooccurrence()
    except Exception as e:
        print(f"âŒ Step 3B failed: {e}")

    # Step 3C: Competitor Analysis (Gemini)
    try:
        results["step3c"] = await test_step3c_competitor_analysis()
    except Exception as e:
        print(f"âŒ Step 3C failed: {e}")

    # Step 5: Primary Sources (simulated)
    try:
        results["step5"] = await test_step5_primary_sources()
    except Exception as e:
        print(f"âŒ Step 5 failed: {e}")

    # Step 6: Enhanced Outline (Anthropic)
    try:
        results["step6"] = await test_step6_enhanced_outline()
    except Exception as e:
        print(f"âŒ Step 6 failed: {e}")

    # Step 6.5: Integration (Anthropic)
    try:
        results["step6_5"] = await test_step6_5_integration()
    except Exception as e:
        print(f"âŒ Step 6.5 failed: {e}")

    # Step 7B: Brush Up (Gemini)
    try:
        results["step7b"] = await test_step7b_brush_up()
    except Exception as e:
        print(f"âŒ Step 7B failed: {e}")

    # Step 8: Fact Check (Gemini)
    try:
        results["step8"] = await test_step8_fact_check()
    except Exception as e:
        print(f"âŒ Step 8 failed: {e}")

    # Step 9: Final Rewrite (Anthropic)
    try:
        results["step9"] = await test_step9_final_rewrite()
    except Exception as e:
        print(f"âŒ Step 9 failed: {e}")

    # Summary
    print_section("Test Summary", "=")
    print(f"Total steps tested: {len(results)}")
    print(f"Steps: {', '.join(results.keys())}")

    # Calculate total tokens
    total_input = sum(
        r.get("tokens", {}).get("input", 0)
        for r in results.values()
        if isinstance(r.get("tokens"), dict)
    )
    total_output = sum(
        r.get("tokens", {}).get("output", 0)
        for r in results.values()
        if isinstance(r.get("tokens"), dict)
    )

    print(f"\nğŸ“Š Total token usage:")
    print(f"   Input tokens: {total_input}")
    print(f"   Output tokens: {total_output}")
    print(f"   Total: {total_input + total_output}")


if __name__ == "__main__":
    asyncio.run(main())
